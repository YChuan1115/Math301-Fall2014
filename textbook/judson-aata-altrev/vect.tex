%%%%(c)
%%%%(c)  This file is a portion of the source for the textbook
%%%%(c)
%%%%(c)    Abstract Algebra: Theory and Applications
%%%%(c)    Copyright 1997 by Thomas W. Judson
%%%%(c)
%%%%(c)  See the file COPYING.txt for copying conditions
%%%%(c)
%%%%(c)
\chap{Vector Spaces}{vect}


 
In a physical system a quantity can often be described with a single
number. For example, we need to know only a single number to describe
temperature, mass, or volume.  However, for some quantities, such as
location, we need several numbers. To give the location of a point in
space, we need $x$, $y$, and $z$ coordinates. Temperature
distribution over a solid object requires four numbers: three to
identify each point within the object and a fourth to describe the
temperature at that point.  Often $n$-tuples of numbers, or vectors,
also have certain algebraic properties, such as addition or scalar 
multiplication.  


In this chapter we will examine mathematical structures called vector
spaces. As with groups and rings, it is desirable to give a simple
list of axioms that must be satisfied to make a set of vectors a
structure worth studying.  
 
 
 
\section{Definitions and Examples}
 

A \boldemph{vector space}\index{Vector space!definition of} $V$ over a
field $F$ is an abelian group with a \boldemph{scalar
product}\index{Scalar product} $\alpha \cdot v$ or $\alpha v$ defined
for all $\alpha \in F$ and all $v \in V$ satisfying the following
axioms.  
\begin{itemize}

\item 
$\alpha(\beta v) =(\alpha \beta)v$;

\item 
$(\alpha + \beta)v =\alpha v + \beta v$;

\item 
$\alpha(u + v) = \alpha u + \alpha v$;

\item 
$1v=v$;

\end{itemize}
where $\alpha, \beta \in F$ and $u, v \in V$.
 

The elements of $V$ are called \boldemph{vectors}; the elements of $F$
are called \boldemph{scalars}.  It is important to notice that in most
cases two vectors cannot be multiplied.  In general, it is only
possible to multiply a vector with a scalar. To differentiate between
the scalar zero and the vector zero, we will write them as 0 and
${\mathbf 0}$, respectively.  


Let us examine several examples of vector spaces. Some of them will be
quite familiar; others will seem less so.
 
 
\begin{example}{vector_space_Rn}
The $n$-tuples of real numbers, denoted by ${\mathbb R}^n$, form a vector
space over ${\mathbb R}$. Given vectors $u = (u_1, \ldots, u_n)$ and $v =
(v_1, \ldots, v_n)$ in ${\mathbb R}^n$ and $\alpha$ in ${\mathbb R}$, we can
define vector addition by
\[
u + v = (u_1, \ldots, u_n) + (v_1, \ldots, v_n)
=
(u_1 + v_1, \ldots, u_n + v_n)
\]
and scalar multiplication by 
\[
\alpha u = \alpha(u_1, \ldots, u_n)= (\alpha u_1, \ldots, \alpha u_n).
\]
\end{example}
 
 
 
\begin{example}{vector_space_Fx}
If $F$ is a field, then $F[x]$ is a vector space over $F$. The vectors
in $F[x]$ are simply polynomials.  Vector addition is just polynomial
addition. If $\alpha \in F$ and $p(x) \in F[x]$, then scalar
multiplication is defined by $\alpha p(x)$.
\end{example}
 
 
\begin{example}{vector_space_cont}
The set of all continuous real-valued functions on a closed interval
$[a,b]$ is a vector space over ${\mathbb R}$.  If $f(x)$ and $g(x)$ are
continuous on $[a, b]$, then $(f+g)(x)$ is defined to be $f(x) +
g(x)$.  Scalar multiplication is defined by
$(\alpha f)(x) = \alpha f(x)$ for $\alpha \in 
{\mathbb R}$. For example, if $f(x) = \sin x$ and $g(x)= x^2$, then 
$(2f+5g)(x) =2 \sin x + 5 x^2$. 
\end{example}
 

 
\begin{example}{vector_space_sqrt2}
Let $V = {\mathbb Q}(\sqrt{2}\, ) = \{ a + b \sqrt{2} : a, b \in 
{\mathbb Q } \}$. Then $V$ is a
vector space over ${\mathbb Q}$. If $u = a + b \sqrt{2}$ and $v = c + d
\sqrt{2}$, then $u + v = (a + c) + (b + d ) \sqrt{2}$ is again in $V$.
Also, for $\alpha \in {\mathbb Q}$, $\alpha v$ is in $V$.  We will leave
it as an exercise to verify that all of the vector space axioms hold
for $V$. 
\end{example}

 
\begin{proposition}
Let $V$ be a vector space over $F$. Then each of the following
statements is true. 
\begin{enumerate}

\rm \item \it 
$0v ={\mathbf 0}$ for all $v \in V$.

\rm \item \it 
$\alpha {\mathbf 0} = {\mathbf 0}$ for all $\alpha \in F$.


\rm \item \it 
If $\alpha v = {\mathbf 0}$, then either $\alpha = 0$ or $v = {\mathbf
0}$.  

\rm \item \it
$(-1) v = -v$ for all $v \in V$.

\rm \item \it 
$-(\alpha v) = (-\alpha)v = \alpha(-v)$ for all $\alpha \in F$ and all
$v \in V$. 

\end{enumerate}
\end{proposition}


\begin{proof}
To prove (1), observe that 
\[
0 v = (0 + 0)v = 0v + 0v;
\]
consequently, ${\mathbf 0} + 0 v = 0v + 0v$. Since $V$ is an abelian
group, ${\mathbf 0} = 0v$. 


The proof of (2) is almost identical to the proof of (1). For (3), we
are done if $\alpha = 0$.  Suppose that $\alpha \neq 0$. Multiplying
both sides of $\alpha v = {\mathbf 0}$ by $1/ \alpha$, we have $v =
{\mathbf 0}$.


To show (4), observe that
\[
v + (-1)v = 1v + (-1)v = (1-1)v = 0v = {\mathbf 0},
\]
and so $-v = (-1)v$. We will leave the proof of (5) as an exercise.
\end{proof}
 
 
 
\section{Subspaces}


Just as groups have subgroups and rings have subrings, vector spaces
also have substructures. Let $V$ be a vector space over a field $F$,
and $W$ a subset of $V$. Then $W$ is a \boldemph{subspace}\index{Vector
space!subspace of} of $V$ if it is closed under vector addition and
scalar multiplication; that is, if $u, v \in W$ and $\alpha
\in F$, it will always be the case that $u+v$ and $\alpha v$ are also
in $W$.   
 

 
\begin{example}{subspace_W}
Let $W$ be the subspace of ${\mathbb R}^3$ defined by $W = \{ (x_1, 2 x_1
+ x_2, x_1 - x_2) : x_1, x_2 \in {\mathbb R} \}$. We claim that $W$ is a 
subspace of ${\mathbb R}^3$.  Since 
\begin{align*}
\alpha (x_1, 2 x_1 + x_2, x_1 - x_2) 
& =  (\alpha x_1, \alpha(2 x_1 + x_2), \alpha( x_1 - x_2)) \\
& =  (\alpha x_1, 2(\alpha x_1) + \alpha x_2, \alpha x_1 -\alpha x_2),
\end{align*}
$W$ is closed under scalar multiplication. To show that $W$ is closed
under vector addition, let $u = (x_1, 2 x_1 + x_2, x_1 - x_2)$ and $v
= (y_1, 2 y_1 + y_2, y_1 - y_2)$ be vectors in $W$. Then
\[
u + v = 
(x_1 + y_1, 2( x_1 + y_1) +( x_2 + y_2), (x_1 + y_1) - (x_2+ y_2)).
\]
\end{example}
 
 
 
\begin{example}{subspace_poly}
Let $W$ be the subset of polynomials of $F[x]$ with no odd-power
terms. If $p(x)$ and $q(x)$ have no odd-power terms, then neither will 
$p(x) + q(x)$.  Also, $\alpha p(x) \in W$ for $\alpha \in F$ and $p(x)
\in W$.
\end{example}
  

% 2010/05/18 R Beezer, "vector field" to "vector space"
Let $V$ be any vector space over a field $F$ and suppose that $v_1,
v_2, \ldots, v_n$ are vectors in $V$ and $\alpha_1, \alpha_2, \ldots,
\alpha_n$ are scalars in $F$. Any vector $w$ in $V$ of the form
\[
w = \sum_{i=1}^n \alpha_i v_i = \alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n
\]
is called a \boldemph{linear combination}\index{Linear combination} of the
vectors $v_1, v_2, \ldots, v_n$. The \boldemph{spanning
set}\index{Spanning set} of vectors $v_1, v_2, \ldots, v_n$ is the
set of vectors obtained from all possible linear combinations of
$v_1, v_2, \ldots, v_n$. If $W$ is the spanning set of $v_1, v_2,
\ldots, v_n$, then we often say that $W$ is \boldemph{spanned} by $v_1,
v_2, \ldots, v_n$. 
 
 
\begin{proposition}
Let $S= \{v_1, v_2, \ldots, v_n \}$ be vectors in a vector space $V$.
Then the span of $S$ is a subspace of $V$. 
\end{proposition}


\begin{proof}
Let $u$ and $v$ be in $S$. We can write both of these vectors as 
linear combinations of the $v_i$'s:
\begin{align*}
u & =  \alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n \\
v & =  \beta_1 v_1 + \beta_2 v_2 + \cdots + \beta_n v_n.
\end{align*}
Then
\[
u+ v =( \alpha_1 + \beta_1) v_1 + (\alpha_2+ \beta_2) v_2 + \cdots +
(\alpha_n + \beta_n) v_n 
\]
is a linear combination of the $v_i$'s. For $\alpha \in F$,
\[
\alpha u = (\alpha \alpha_1) v_1 + ( \alpha \alpha_2) v_2 + \cdots +
(\alpha \alpha_n ) v_n 
\]
is in the span of $S$.
\end{proof}


 
\section{Linear Independence}
 

Let $S = \{v_1, v_2, \ldots, v_n\}$ be a set of vectors in a vector
space $V$. If there exist scalars $\alpha_1, \alpha_2 \ldots \alpha_n
\in F$ such that not all of the $\alpha_i$'s are zero and 
\[
\alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n = {\mathbf 0 },
\]
then $S$ is said to be \boldemph{linearly
dependent}\index{Linear dependence}. If the set $S$ is not linearly
dependent, then it is said to be \boldemph{linearly
independent}\index{Linear independence}. More specifically, $S$ is a
linearly independent set if
\[ 
\alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n = {\mathbf 0 }
\]
implies that
\[
\alpha_1 = \alpha_2 = \cdots = \alpha_n = 0
\]
for any set of scalars $\{ \alpha_1, \alpha_2 \ldots \alpha_n \}$.


 
\begin{proposition}
Let $\{ v_1, v_2, \ldots, v_n \}$ be a set of linearly independent
vectors in a vector space. Suppose that 
\[
v = \alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n
= \beta_1 v_1 + \beta_2 v_2 + \cdots + \beta_n v_n.
\]
Then $\alpha_1 = \beta_1, \alpha_2 = \beta_2, \ldots, \alpha_n =
\beta_n$. 
\end{proposition}

\begin{proof}
If 
\[
v = \alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n
= \beta_1 v_1 + \beta_2 v_2 + \cdots + \beta_n v_n,
\]
then
\[
(\alpha_1 - \beta_1) v_1 + (\alpha_2 - \beta_2) v_2 + \cdots +
(\alpha_n - \beta_n) v_n = {\mathbf 0}.
\]
Since $v_1, \ldots, v_n$ are linearly independent, $\alpha_i - \beta_i
=0$ for $i = 1, \ldots, n$.
\end{proof}
 

\medskip


The definition of linear dependence makes more sense if we consider
the following proposition.

 
\begin{proposition}
A set $\{ v_1, v_2, \dots, v_n \}$ of vectors in a vector space $V$ is
linearly dependent if and only if one of the $v_i$'s is a linear
combination of the rest. 
\end{proposition}


\begin{proof}
Suppose that $\{ v_1, v_2, \dots, v_n \}$ is a set of linearly dependent
vectors.  Then there exist scalars $\alpha_1, \ldots, \alpha_n$
such that
\[
\alpha_1 v_1 + \alpha_2 v_2 + \cdots + \alpha_n v_n = {\mathbf 0 },
\]
with at least one of the $\alpha_i$'s not equal to zero.  Suppose that
$\alpha_k \neq 0$. Then 
\[
v_k = - \frac{\alpha_1}{\alpha_k} v_1 
- \cdots 
- \frac{\alpha_{k-1}}{\alpha_k}	v_{k-1}
- \frac{\alpha_{k+1}}{\alpha_k}	v_{k+1}
- \cdots 
- \frac{\alpha_n}{\alpha_k} v_n.
\]


Conversely, suppose that 
\[
v_k = \beta_1 v_1 
+ \cdots 
+ \beta_{k-1} v_{k-1}
+ \beta_{k+1} v_{k+1}
+ \cdots 
+ \beta_n v_n.
\]
Then
\[
\beta_1 v_1 
+ \cdots 
+ \beta_{k-1} v_{k-1}
- v_k
+ \beta_{k+1} v_{k+1}
+ \cdots 
+ \beta_n v_n = {\mathbf 0}.
\]
\end{proof}

\medskip


The following proposition is a consequence of the fact that any system
of homogeneous linear equations with more unknowns than equations will
have a nontrivial solution.  We leave the details of the proof for the
end-of-chapter exercises. 
 

\begin{proposition}\label{vect:linearlyindependent}
Suppose that a vector space $V$ is spanned by $n$ vectors. If $m > n$,
then any set of $m$ vectors in $V$ must be linearly dependent. 
\end{proposition}
 
  
A set $\{ e_1, e_2, \ldots, e_n \}$ of vectors in a vector space $V$
is called a \boldemph{basis}\index{Vector space!basis of} for $V$ if $\{
e_1, e_2, \ldots, e_n \}$ is a linearly independent set that spans
$V$.

 
 
\begin{example}{basis_R3}
The vectors $e_1 = (1, 0, 0)$, $e_2 = (0, 1, 0)$, and $e_3 =(0, 0, 1)$
form a basis for ${\mathbb R}^3$.  The set certainly spans ${\mathbb R}^3$,
since any arbitrary vector $(x_1, x_2, x_3)$ in ${\mathbb R}^3$ can be
written as $x_1 e_1 + x_2 e_2 + x_3 e_3$. Also, none of the vectors
$e_1, e_2, e_3$ can be written as a linear combination of the other
two; hence, they are linearly independent.  The vectors $e_1, e_2,
e_3$ are not the only basis of ${\mathbb R}^3$:  the set $\{ (3, 2, 1),
(3, 2, 0), (1, 1, 1) \}$ is also a basis for ${\mathbb R}^3$. 
\end{example}

 
 
\begin{example}{basis_sqrt2}
Let ${\mathbb Q}( \sqrt{2}\, ) = \{ a + b \sqrt{2} : a, b \in {\mathbb Q} \}$.
The sets $\{1, \sqrt{2}\,  \}$ and $\{1+\sqrt{2}, 1- \sqrt{2}\,  \}$ are
both bases of ${\mathbb Q}( \sqrt{2}\, )$.  
\end{example}





From the last two examples it should be clear that a given vector
space has several bases. In fact, there are an infinite number of
bases for both of these examples. \emph{In general, there is no unique 
basis for a vector space}.  However, every basis of ${\mathbb R}^3$ consists
of exactly three vectors, and every  basis of ${\mathbb Q}(\sqrt{2}\, )$ 
consists of exactly two vectors. This is a consequence of the next 
proposition.


\begin{proposition}
Let $\{ e_1, e_2, \ldots, e_m \}$ and $\{ f_1, f_2, \ldots, f_n \}$ be
two bases for a vector space $V$. Then $m=n$. 
\end{proposition}


\begin{proof}
Since $\{ e_1, e_2, \ldots, e_m \}$ is a basis, it is a linearly
independent set.  By  Proposition~\ref{vect:linearlyindependent}, $n \leq m$. Similarly, $\{
f_1, f_2, \ldots, f_n \}$ is a linearly independent set, and the last
proposition implies that $m \leq n$.  Consequently, $m =n$.
\mbox{\hspace{1in}}
\end{proof}
%Label repaired.  Suggested by R. Beezer.
%TWJ - 12/19/2011
 

\medskip
 

If $\{ e_1, e_2, \ldots, e_n \}$ is a basis for a vector space $V$,
then we say that the \boldemph{dimension}\index{Vector space!dimension of}
of $V$ is $n$ and we write $\dim V =n$\label{vectdim}. 
We will leave the proof of the following theorem as an exercise.


\begin{theorem}
Let $V$ be a vector space of dimension $n$.
\begin{enumerate}

\rm \item \it
If $S = \{v_1, \ldots, v_n \}$ is a set of linearly independent
vectors for $V$, then $S$ is a basis for $V$. 

\rm \item \it
If $S = \{v_1, \ldots, v_n \}$ spans $V$, then $S$ is a basis for $V$. 

\rm \item \it
If $S = \{v_1, \ldots, v_k \}$ is a set of linearly independent
vectors for $V$ with $k < n$, then there exist vectors $v_{k+1},
\ldots, v_n$ such that  
\[
\{v_1, \ldots, v_k, v_{k+1}, \ldots, v_n \}
\]
is a basis for $V$. 

\end{enumerate}
\end{theorem}


 
\markright{EXERCISES}
\section*{Exercises}
\exrule

 
{\small
\begin{enumerate}

  
\item
If $F$ is a field, show that $F[x]$ is a vector space over $F$, where
the vectors in $F[x]$ are polynomials.  Vector addition is polynomial
addition, and scalar multiplication is defined by $\alpha p(x)$ for
$\alpha \in F$.  

\item
Prove that ${\mathbb Q }( \sqrt{2}\, )$ is a vector space.


\item
Let ${\mathbb Q }( \sqrt{2}, \sqrt{3}\, )$ be the field generated by
elements of the form $a + b \sqrt{2}  + c \sqrt{3}$, where $a, b, c$
are in ${\mathbb Q}$. Prove that ${\mathbb Q }( \sqrt{2}, \sqrt{3}\, )$ is a
vector space of dimension 4 over ${\mathbb Q}$.  Find a basis for 
${\mathbb Q }( \sqrt{2}, \sqrt{3}\, )$.


\item 
Prove that the complex numbers are a vector space of dimension 2
over ${\mathbb R}$. 


\item
Prove that the set $P_n$ of all polynomials of degree less than $n$
form a subspace of the vector space $F[x]$. Find a basis for $P_n$ and
compute the dimension of~$P_n$. 


\item
Let $F$ be a field and denote the set of $n$-tuples of $F$ by $F^n$.
Given vectors $u = (u_1, \ldots, u_n)$ and $v = (v_1, \ldots, v_n)$ in
$F^n$ and $\alpha$ in $F$, define vector addition by
\[
u + v = (u_1, \ldots, u_n) + (v_1, \ldots, v_n)
=
(u_1 + v_1, \ldots, u_n + v_n)
\]
and scalar multiplication by 
\[
\alpha u = \alpha(u_1, \ldots, u_n)= (\alpha u_1, \ldots, \alpha u_n).
\]
Prove that $F^n$ is a vector space of dimension $n$ under these
operations. 


\item
Which of the following sets are subspaces of ${\mathbb R}^3$? If the set
is indeed a subspace, find a basis for the subspace and compute its
dimension.
\begin{enumerate}

  \item
$\{ (x_1, x_2, x_3) : 3 x_1 - 2 x_2 + x_3 = 0 \}$

  \item
$\{ (x_1, x_2, x_3) : 3 x_1 + 4 x_3 = 0, 2 x_1 - x_2 + x_3 = 0 \}$

  \item
$\{ (x_1, x_2, x_3) : x_1 - 2 x_2 + 2 x_3 = 2 \}$

  \item
$\{ (x_1, x_2, x_3) : 3 x_1 - 2 x_2^2 = 0 \}$

\end{enumerate}


\item
Show that the set of all possible solutions $(x, y, z) \in {\mathbb R}^3$
of the equations
\begin{align*}
Ax + B y + C z & =  0 \\
D x + E y + C z & =  0
\end{align*}
form a subspace of ${\mathbb R}^3$.


\item
Let $W$ be the subset of continuous functions on $[0, 1]$ such that
$f(0) = 0$.  Prove that $W$ is a subspace of $C[0, 1]$.


%*******************THEORY***********************


\item
Let $V$ be a vector space over $F$. Prove that $-(\alpha v) =
(-\alpha)v = \alpha(-v)$ for all $\alpha \in F$ and all $v \in V$. 


\item
Let $V$ be a vector space of dimension $n$. Prove each of the
following statements. 
\begin{enumerate}

 \item
If $S = \{v_1, \ldots, v_n \}$ is a set of linearly independent
vectors for $V$, then $S$ is a basis for $V$. 

 \item
If $S = \{v_1, \ldots, v_n \}$ spans $V$, then $S$ is a basis for $V$.

 \item 
If $S = \{v_1, \ldots, v_k \}$ is a set of linearly independent
vectors for $V$ with $k < n$, then there exist vectors $v_{k+1},
\ldots, v_n$ such that 
\[
\{v_1, \ldots, v_k, v_{k+1}, \ldots, v_n \}
\]
is a basis for $V$. 

\end{enumerate}


\item
Prove that any set of vectors containing ${\mathbf 0}$ is linearly
dependent. 


\item
Let $V$ be a vector space. Show that $\{ {\mathbf 0} \}$ is a subspace
of $V$ of dimension zero.


\item
If a vector space $V$ is spanned by $n$ vectors, show that any set of
$m$ vectors in $V$ must be linearly dependent for $m > n$.  


\item \label{vect:linear_transformation}
\textbf{Linear Transformations.}
Let $V$ and $W$ be vector spaces over a field $F$, of dimensions $m$
and $n$, respectively. If $T: V \rightarrow W$ is a map satisfying
\begin{align*}
T( u+ v ) & =  T(u ) + T(v) \\
T( \alpha v ) & =  \alpha T(v)
\end{align*}
for all $\alpha \in F$ and all $u, v \in V$, then $T$ is called a
\boldemph{linear transformation}\index{Linear transformation!definition
of} from $V$ into $W$. 
\begin{enumerate}

   \item
Prove that the \boldemph{kernel}\index{Kernel!of a linear
transformation}\index{Linear transformation!kernel of} of $T$, 
$\ker(T) = \{ v \in V : T(v) = 
{\mathbf 0} \}$, is a subspace of $V$. The kernel of $T$ is sometimes
called the \boldemph{null space}\index{Null space!of a linear
transformation}\index{Linear transformation!null space of} of $T$. 

   \item
Prove that the \boldemph{range}\index{Linear transformation!range of} or
\boldemph{range space} of $T$, $R(V) = \{ w \in W : T(v) = w \text{ for
some } v \in V \}$, is a subspace of $W$.

   \item
Show that $T : V \rightarrow W$ is injective if and only if 
$\ker(T) = \{ \mathbf 0 \}$.

   \item
Let $\{ v_1, \ldots, v_k \}$ be a basis for the null space of $T$. We
can extend this basis to be a basis $\{ v_1, \ldots, v_k, v_{k+1},
\ldots, v_m\}$ of $V$. Why?  Prove that $\{ T(v_{k+1}), \ldots, T(v_m)
\}$ is a basis for the range of $T$. Conclude that the range of $T$
has dimension $m-k$.

   \item
Let $\dim V = \dim W$.  Show that a linear transformation $T : V
\rightarrow W$ is injective if and only if it is surjective.

\end{enumerate}


\item
Let $V$ and $W$ be finite dimensional vector spaces of dimension $n$
over a field $F$. Suppose that  $T: V \rightarrow W$ is a vector space
isomorphism.  If $\{ v_1, \ldots, v_n \}$ is a basis of $V$, show that
$\{ T(v_1), \ldots, T(v_n) \}$ is a basis of $W$. Conclude that any
vector space over a field $F$ of dimension $n$ is isomorphic to $F^n$. 


\item
\textbf{Direct Sums.} 
Let $U$ and $V$ be subspaces of a vector space $W$. The sum of $U$ and
$V$, denoted $U + V$, is defined to be the set of all vectors of the
form $u + v$, where $u \in U$ and $v \in V$. 
\begin{enumerate}

   \item
Prove that $U + V$ and $U \cap V$ are subspaces of $W$.

   \item
If $U + V = W$ and $U \cap V = {\mathbf 0}$, then $W$ is said to be the
\boldemph{direct sum}\index{Direct sum of vector spaces}\index{Vector
space!direct sum of} of $U$ and $V$ and we write $W = U \oplus
V$\label{notedirectsum}.
Show that every element $w \in W$ can be written uniquely as $w = u +
v$, where $u \in U$ and $v \in V$.

   \item
Let $U$ be a subspace of dimension $k$ of a vector space $W$ of
dimension $n$. Prove that there exists a subspace $V$ of dimension
$n-k$ such that $W = U \oplus V$.  Is the subspace $V$ unique?

   \item
If $U$ and $V$ are arbitrary subspaces of a vector space $W$, show
that 
\[
\dim( U + V) = \dim U + \dim V - \dim( U \cap V).
\]

\end{enumerate}


\item
\textbf{Dual Spaces.} 
Let $V$ and $W$ be finite dimensional vector spaces over a field~$F$. 
\begin{enumerate}

   \item
Show that the set of all linear transformations from $V$ into $W$,
denoted by $\Hom(V, W)$\label{noteHom}, 
is a vector space over $F$, where we
define vector addition as follows:
\begin{align*}
(S + T)(v) &=  S(v) +T(v) \\
(\alpha S)(v) & =  \alpha S(v),
\end{align*}
where $S, T \in \Hom(V, W)$, $\alpha \in F$, and $v \in V$.
 
   \item
Let $V$ be an $F$-vector space.  Define the \boldemph{dual
space}\index{Vector space!dual of} of $V$ 
to be $V^\ast = \Hom(V, F)$\label{notedual}. Elements in the dual space of $V$ are
called \boldemph{linear functionals}\index{Linear functionals}.  Let $v_1,
\ldots, v_n$ be an ordered basis for $V$. If $v = \alpha_1 v_1 +
\cdots + \alpha_n v_n$ is any vector in $V$, define a linear
functional  $\phi_i : V \rightarrow F$ by $\phi_i (v) = \alpha_i$.
Show that the $\phi_i$'s form a basis for $V^\ast$.  This basis is
called the \boldemph{dual basis} of $v_1, \ldots, v_n$ (or simply the dual
basis if the context makes the meaning clear).  
 

   \item
Consider the basis $\{ (3, 1), (2, -2) \}$ for ${\mathbb R}^2$. What is
the dual basis for $({\mathbb R}^2)^\ast$? 
 
   \item
Let $V$ be a vector space of dimension $n$ over a field $F$ and let
$V^{\ast \ast}$ be the dual space $V^\ast$.  Show that each element $v
\in V$ gives rise to an element $\lambda_v$ in $V^{\ast \ast}$ and
that the map $v \mapsto \lambda_v$ is an isomorphism of $V$ with
$V^{\ast \ast}$. 

\end{enumerate}
 

\end{enumerate}
}


 
\subsection*{References and Suggested Readings}  %References updated - TWJ 8/19/2010
 

{\small
\begin{itemize}

\item[\textbf{[1]}] %Reference added - TWJ 8/19/2010
Beezer, R. \textit{A First Course in Linear Algebra}.
Available online at \\
{\tt http://linear.ups.edu/}.  2004.

\item[\textbf{[2]}] %Reference added - TWJ 8/19/2010
Bretscher, O. \textit{Linear Algebra with Applications}. 4th ed.
Pearson, Upper Saddle River, NJ, 2009.
 
\item[\textbf{[3]}]  
Curtis, C. W. \textit{Linear Algebra: An Introductory Approach}. 4th ed.
Springer, New York, 1984.
 
\item[\textbf{[4]}]
Hoffman, K. and Kunze, R. \textit{Linear Algebra}. 2nd ed.
Prentice-Hall, Englewood Cliffs, NJ, 1971.

\item[\textbf{[5]}] %Reference updated.  Not yet published. - TWJ 8/19/2010
Johnson, L. W., Riess, R. D., and Arnold, J. T. \textit{Introduction to
Linear Algebra}. 6th ed. 
Pearson, Upper Saddle River, NJ, 2011.
 
\item[\textbf{[6]}] %Reference updated - TWJ 8/19/2010
Leon, S. J. \textit{Linear Algebra with Applications}. 8th ed.
Pearson, Upper Saddle River, NJ, 2010.
 

\end{itemize}
}

\sagesection


